{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Start Mech Int Investigation #23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore how the model is doing prediction of the first path token in the maze (ie. the token that immediately comes after the `PATH_START` token), to this end we use:\n",
    "\n",
    "* Logit Lens\n",
    "* Direct Logit Attribution\n",
    "* Activation Patching\n",
    "\n",
    "In summary, we find that the majority of computation associated with this task is in the form of MLP computation (in particular in MLP10 and MLP11) but that there does appear to be a few heads within early layers (Layer 0, 1 and 2) that are also playing some role. Further evals outside of this notebook however are suggestive of this instance of maze-transformer doing some form of memorization (atleast overfitting to its training data and not generalising OOD), for this reason, this study has not yet been taken further.\n",
    "\n",
    "This notebook takes significant inspiration from two great resources:\n",
    "\n",
    "* Nanda's [Exploratory Analysis Demo](https://github.com/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb) Notebook\n",
    "* Janiak & Heimersheim's [Python docstrings](https://colab.research.google.com/drive/17CoA1yARaWHvV14zQGcI3ISz1bIRZKS5) Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import typing\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import einops\n",
    "import pandas as pd\n",
    "from fancy_einsum import einsum\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn import preprocessing\n",
    "from jaxtyping import Float, Int\n",
    "# from torchtyping import TensorType as TT\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plotting\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "# import seaborn\n",
    "# from neel_plotly import imshow, line\n",
    "\n",
    "# TransformerLens\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "from transformer_lens import utils as et_utils\n",
    "from transformer_lens import utils\n",
    "\n",
    "# muutils\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "# Our Code\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig, SolvedMaze, LatticeMaze, SPECIAL_TOKENS\n",
    "from maze_dataset.plotting import plot_dataset_mazes\n",
    "from maze_transformer.tokenizer import HuggingMazeTokenizer\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig\n",
    "\n",
    "# from prelim_mech_int import *\n",
    "\n",
    "pio.renderers.default = 'vscode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1d738559de0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Notebook\n",
    "device = configure_notebook(seed=42, dark_mode=True)\n",
    "\n",
    "# We won't be training any models\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model in\n",
    "\n",
    "We can start by loading the model into the notebook, as well as a dataset of 20,000 mazes. This instance of maze-transformer is capable of solving 6x6 mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with 1.3M params (num_params = 1274699)\n"
     ]
    }
   ],
   "source": [
    "MODEL: ZanjHookedTransformer = ZanjHookedTransformer.read(\"../examples/hallway-medium_2023-06-16-03-40-47.iter_26554.zanj\")\n",
    "num_params: int = MODEL.num_params()\n",
    "print(f\"loaded model with {shorten_numerical_to_str(num_params)} params ({num_params = })\")\n",
    "GPT_CONFIG: BaseGPTConfig = MODEL.zanj_model_config.model_cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 mazes and pass into model, storing logits and cache\n",
    "n_examples: int = 10\n",
    "# get a dataset from the same config as the model -- you can also pass in a new config\n",
    "TEST_DATASET_CFG: MazeDatasetConfig = deepcopy(MODEL.zanj_model_config.dataset_cfg)\n",
    "TEST_DATASET_CFG.n_mazes = n_examples\n",
    "DATASET: MazeDataset = MazeDataset.from_config(TEST_DATASET_CFG)\n",
    "DATASET_TOKENS_UNJOINED: list[list[str]] = DATASET.as_tokens(join_tokens_individual_maze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 1500x500 with 10 Axes>,\n",
       " array([<Axes: >, <Axes: >, <Axes: >, <Axes: >, <Axes: >, <Axes: >,\n",
       "        <Axes: >, <Axes: >, <Axes: >, <Axes: >], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dataset_mazes(DATASET, count=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some further clarity about what we are trying to find\n",
    "Now that we have a dataset in the notebook, lets just first demonstrate what behaviour we are trying to understand. Let't take the first maze from the maze dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ADJLIST_START> (3,7) <--> (3,6) ; (6,5) <--> (5,5) ; (5,5) <--> (5,4) ; (5,3) <--> (5,2) ; (4,1) <--> (5,1) ; (3,5) <--> (3,6) ; (4,6) <--> (4,7) ; (4,4) <--> (4,5) ; (5,2) <--> (5,1) ; (5,4) <--> (5,3) ; (3,7) <--> (4,7) ; (6,5) <--> (6,4) ; (4,6) <--> (4,5) ; (4,1) <--> (4,2) ; (3,3) <--> (3,4) ; (4,3) <--> (3,3) ; (3,5) <--> (3,4) ; (4,3) <--> (4,2) ; (6,3) <--> (6,4) ; <ADJLIST_END> <ORIGIN_START> (3,4) <ORIGIN_END> <TARGET_START> (4,1) <TARGET_END> <PATH_START> (3,4) (3,3) (4,3) (4,2) (4,1) <PATH_END>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" \".join(DATASET[0].as_tokens(DATASET.cfg.node_token_map)))\n",
    "plt.imshow(DATASET[0].as_pixels())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the correct prediction following `<PATH_START>` is the position ~~`(5, 0)`~~. The question we are trying to answer is, given examples of the form above (but with data truncated to the `PATH_START` and some padding at the start to make consistent lengths) how does the model predict ~~`(5, 0)`~~ over any other token?\n",
    "\n",
    "To explore this behaviour, let's truncate and pad the full dataset of examples we have pulled in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing\n",
    "\n",
    "we want to, for each maze, get a list of context tokens and a list of target tokens. For this example, we get the context tokens by simply looking for the first index of the path start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContextGetterFunction = typing.Callable[[list[str]], int]\n",
    "\n",
    "def get_token_first_index_factory(token: str) -> ContextGetterFunction:\n",
    "\treturn lambda token_list: token_list.index(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_context_target_pairs(\n",
    "    dataset_tokens: list[list[str]],\n",
    "    context_getter: ContextGetterFunction,\n",
    ") -> tuple[list[list[str]], list[str]]:\n",
    "\n",
    "\toutput_contexts: list[list[str]] = list()\n",
    "\toutput_targets: list[str] = list()\n",
    "\tfor maze in dataset_tokens:\n",
    "\t\tindex: int = context_getter(maze)\n",
    "\t\toutput_contexts.append(maze[:index])\n",
    "\t\toutput_targets.append(maze[index])\n",
    "\n",
    "\treturn output_contexts, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_START_CONTEXT_GETTER: ContextGetterFunction = get_token_first_index_factory(SPECIAL_TOKENS[\"path_start\"])\n",
    "\n",
    "DATASET_CONTEXTS: list[list[str]]; DATASET_TARGETS: list[str]\n",
    "DATASET_CONTEXTS, DATASET_TARGETS = process_dataset_context_target_pairs(\n",
    "    DATASET_TOKENS_UNJOINED,\n",
    "    context_getter=lambda token_list: PATH_START_CONTEXT_GETTER(token_list) + 1,\n",
    ")\n",
    "\n",
    "DATASET_CONTEXTS_JOINED: list[str] = [\" \".join(context) for context in DATASET_CONTEXTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: <ADJLIST_START> (6,4) <--> (6,3) ; (4,7) <--> (3,7) ; (3,4) <--> (3,5) ; (3,3) <--> (4,3) ; (4,4) <--> (4,5) ; (3,7) <--> (3,6) ; (3,3) <--> (3,4) ; (4,7) <--> (4,6) ; (4,2) <--> (4,1) ; (6,5) <--> (5,5) ; (3,5) <--> (3,6) ; (4,6) <--> (4,5) ; (5,1) <--> (4,1) ; (6,4) <--> (6,5) ; (5,1) <--> (5,2) ; (5,2) <--> (5,3) ; (5,3) <--> (5,4) ; (4,3) <--> (4,2) ; (5,4) <--> (5,5) ; <ADJLIST_END> <ORIGIN_START> (3,4) <ORIGIN_END> <TARGET_START> (4,1) <TARGET_END> <PATH_START>\n",
      "target: (6,4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"context: {' '.join(DATASET_CONTEXTS[0])}\")\n",
    "print(f\"target: {DATASET_TARGETS[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first question is, how good is the model at predicting the **correct** first token after `PATH_START` in each maze example?\n",
    "\n",
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the model predict on the maze examples, storing logits and activations in cache\n",
    "with torch.no_grad():\n",
    "\tLOGITS, CACHE = MODEL.run_with_cache(DATASET_CONTEXTS_JOINED, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits are in the shape torch.Size([10, 221, 75])\n",
      "This corresponds to [batch_size, num_tokens, vocab_size]\n"
     ]
    }
   ],
   "source": [
    "print(f'Logits are in the shape {LOGITS.shape}')\n",
    "print(f'This corresponds to [batch_size, num_tokens, vocab_size]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict on the next token (the first path coordinate), thus logits associated with final token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction are of the shape: torch.Size([10])\n",
      "This makes sense, we want one prediction for each example in the batch\n"
     ]
    }
   ],
   "source": [
    "# Get the last token prediction from the model\n",
    "last_token_logits = LOGITS[:, -1, :]\n",
    "predictions = []\n",
    "for sample in last_token_logits:\n",
    "    last_token_pred = torch.argmax(sample).item()\n",
    "    predictions.append(last_token_pred)\n",
    "predictions = torch.tensor(predictions)\n",
    "print(f'Prediction are of the shape: {predictions.shape}')\n",
    "print('This makes sense, we want one prediction for each example in the batch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a quick look at what these predictions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 56, 60, 36, 23, 21, 48, 20, 44, 43])\n",
      "\n",
      "Converting this back to text gives us: ['(3,4)', '(6,4)', '(0,7)', '(0,5)', '(2,3)', '(3,0)', '(2,6)', '(0,3)', '(1,5)', '(5,4)']\n",
      "The targets, for comparison:           ['(3,4)', '(6,4)', '(0,7)', '(0,5)', '(2,3)', '(3,0)', '(2,6)', '(0,3)', '(1,5)', '(5,4)']\n"
     ]
    }
   ],
   "source": [
    "print(f'{predictions}\\n')\n",
    "print(f'Converting this back to text gives us: {MODEL.to_str_tokens(predictions)}')\n",
    "print(f'The targets, for comparison:           {DATASET_TARGETS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see what prediction the model is giving based on each maze example in the dataset. Let's see if these predictions match the *ground truth values*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by comparing the above predictions to these ground truths, the model is indeed very good at predicting the first token after `<PATH_START>`. Let's compare these a little bit more systematically. \n",
    "\n",
    "We need to compare the logits associated with the ground truth token to some other token in the dataset, a number of tokens make sense here (ie. a random one, maybe an average over all). However, I think that this token should represent a 'reasonable' wrong response, in this case the most plausible token to me currently seems to be the next token in the dataset (ie. the second token on the ground truth path of each maze). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tok_on_path = []\n",
    "for maze in mazes_tokens:\n",
    "    for idx, tok in enumerate(maze):\n",
    "        if tok == 6:\n",
    "            second_tok_on_path.append(maze[idx+2])\n",
    "            \n",
    "second_tok_on_path_tokens = torch.stack(second_tok_on_path).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Second tokens on the path are: {second_tok_on_path_tokens}\\n')\n",
    "print(f'Converting this back to text gives us: {model.to_str_tokens(second_tok_on_path_tokens)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of correct tokens (our ground truths for each maze example) and a set of incorrect tokens (in this case just every second token on the maze example's path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects_incorrects = []\n",
    "for i in range(len(ground_truth_tokens)):\n",
    "    corrects_incorrects.append([ground_truth_tokens[i].item(), second_tok_on_path_tokens[i].item()])\n",
    "correct_incorrect_indices = torch.tensor(corrects_incorrects).long().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the logit difference across these incorrect and correct tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Neels explanatory notebook: https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(logits, correct_incorrect_indices, per_prompt=True))\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(logits, correct_incorrect_indices)\n",
    "print(\"Average logit difference:\", logits_to_ave_logit_diff(logits, correct_incorrect_indices).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit difference across the batch is high, it can be interpreted as the model being $e^{11.3}$ more likely to choose the correct repsonse over the incorrect response. Another metric to compare the model's ability is accuracy, defining this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, ground_truths):\n",
    "    return f'{(predictions.to(\"cpu\") == ground_truths.to(\"cpu\")).sum().item() / len(predictions.to(\"cpu\")):.0%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logits = logits[:, -1, :]\n",
    "answer_logits = final_logits.gather(dim=-1, index=correct_incorrect_indices)\n",
    "preds = torch.argmax(final_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check accuracy of the predictions against the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(preds, ground_truth_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use logit lens to decompose the residual stream into contribution of logit difference from individual components. This is done by using the unembedding matrix after each block in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the correct and incorrect values into the residual stream\n",
    "correct_incorrect_residual_directions = model.tokens_to_residual_directions(correct_incorrect_indices)\n",
    "print(f\"Correct & Incorrect residual directions shape: {correct_incorrect_residual_directions.shape}\")\n",
    "\n",
    "diff_residual_directions = correct_incorrect_residual_directions[:, 0, :] - correct_incorrect_residual_directions[:, 1, :]\n",
    "print(f\"Correct - Incorrect diff residual directions shape: {diff_residual_directions.shape}\")\n",
    "\n",
    "# cache the values at the end of the residual stream\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "\n",
    "# Get the final token resid stream values (like we did above with last_pred_token)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "\n",
    "# Scaling the values in residual stream with layer norm\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer = -1, pos_slice=-1)\n",
    "\n",
    "# Average logit diff from residual stream method\n",
    "average_logit_diff = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream, diff_residual_directions) / n_examples\n",
    "print(\"Calculated average logit diff:\", average_logit_diff.item())\n",
    "print(\"Original logit difference:\",original_average_logit_diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(residual_stack: Float[torch.Tensor, \"components batch d_model\"], cache: ActivationCache) -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer = -1, pos_slice=-1)\n",
    "    return einsum(\"... batch d_model, batch d_model -> ...\", scaled_residual_stack, diff_residual_directions)/n_examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the logit difference from the accumulated residual stream, this means we are looking at the logit difference between the correct at: \n",
    "1. Layer 0 resid_mid, \n",
    "2. Accumulated across layer 0 resid_mid + layer 0 resid_post, \n",
    "3. Accumulated across layer 0 resid_mid + layer 0 resid_post + layer 1 resid_mid,\n",
    "4. etc.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Accumulated logit difference from entire resid stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "line(logit_lens_logit_diffs, x=np.arange(model.cfg.n_layers*2+1)/2, hover_name=labels, title=\"Logit Difference From Accumulated Residual Stream\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that model seems to be largely unable to do this task until around layer 8, we also see a signficant contribution between layer 11 resid_mid and layer 11 resid_post (indicating a large influence of the Layer 11 MLP on this task).\n",
    "\n",
    "We can also look at the logit difference contribution for layer in isolation (removing the cumulative effect of previous layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this suggesting:\n",
    "* MLP11 is really important and MLP8, MLP9 and MLP10 are quite important\n",
    "\n",
    "* It doesnt appear to be the case that attention is very important"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution\n",
    "\n",
    "We now move onto applying the same techique but at the level of individual heads and neurons (as opposed to entire layer or blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of Jett's plotting functions from here: https://github.com/jettjaniak/mi_utils_public/blob/main/plotting.py\n",
    "def imshow(\n",
    "    tensor,\n",
    "    xlabel=\"X\",\n",
    "    ylabel=\"Y\",\n",
    "    zlabel=None,\n",
    "    xticks=None,\n",
    "    yticks=None,\n",
    "    c_midpoint=0.0,\n",
    "    c_scale=\"RdBu\",\n",
    "    show=True,\n",
    "    **kwargs\n",
    "):\n",
    "    tensor = et_utils.to_numpy(tensor)\n",
    "    if \"animation_frame\" not in kwargs:\n",
    "        assert len(tensor.shape) == 2\n",
    "    else:\n",
    "        assert len(tensor.shape) == 3\n",
    "    xticks = xticks or range(tensor.shape[-1])\n",
    "    yticks = yticks or range(tensor.shape[-2])\n",
    "    xticks = [str(x) for x in xticks]\n",
    "    yticks = [str(y) for y in yticks]\n",
    "    if len(xticks) != len(set(xticks)):\n",
    "        xticks = [f\"{i}_{x}\" for i, x in enumerate(xticks)]\n",
    "    if len(yticks) != len(set(yticks)):\n",
    "        yticks = [f\"{i}_{y}\" for i, y in enumerate(yticks)]\n",
    "    labels = {\"x\": xlabel, \"y\": ylabel}\n",
    "    if zlabel is not None:\n",
    "        labels[\"color\"] = zlabel\n",
    "    fig = px.imshow(\n",
    "        et_utils.to_numpy(tensor),\n",
    "        x=xticks,\n",
    "        y=yticks,\n",
    "        labels=labels,\n",
    "        color_continuous_midpoint=c_midpoint,\n",
    "        color_continuous_scale=c_scale,\n",
    "        **kwargs\n",
    "    )\n",
    "    if show:\n",
    "        fig.show()\n",
    "    else:\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(per_head_logit_diffs, \"(layer head_index) -> layer head_index\", layer=model.cfg.n_layers, head_index=model.cfg.n_heads)\n",
    "imshow(per_head_logit_diffs, xlabel = \"Heads\", ylabel = \"Layer\", title=\"Logit Difference From Each Head\", aspect=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are suggestive of various heads (those in blue) having a positive effect on logit difference, their effect however is relatively small (only amounting for example to a 0.05 increase in logits for the correct response vs the incorrect response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_neuron_residual, labels = cache.stack_neuron_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_neuron_logit_diffs = residual_stack_to_logit_diff(per_neuron_residual, cache)\n",
    "per_neuron_logit_diffs = einops.rearrange(per_neuron_logit_diffs, \"(layer neuron_index) -> layer neuron_index\", layer=model.cfg.n_layers, neuron_index=model.cfg.d_mlp)\n",
    "imshow(per_neuron_logit_diffs, xlabel = 'Neuron Index', ylabel = \"Layer\", aspect = \"auto\", title=\"Logit Difference From Each Neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.01\n",
    "top_neuron_logit_diffs = []\n",
    "per_neuron_logit_diffs = per_neuron_logit_diffs.to(\"cpu\").numpy()\n",
    "for idx, layer in enumerate(per_neuron_logit_diffs):\n",
    "    top_sorted_layer = (np.sort(layer)[-int(percentile*len(layer)):])\n",
    "    top_of_layer = []\n",
    "    for idx, neuron in enumerate(layer):\n",
    "        if neuron in top_sorted_layer:\n",
    "            top_of_layer.append((int(idx), neuron))\n",
    "    top_neuron_logit_diffs.append(top_of_layer)\n",
    "top_neuron_logit_diffs = np.array(top_neuron_logit_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(top_neuron_logit_diffs[:, :, 1],\n",
    "                y = [y for y in range(0, model.cfg.n_layers, 1)], \n",
    "                color_continuous_scale='RdBu', \n",
    "                color_continuous_midpoint=0.0, \n",
    "                aspect=\"auto\",\n",
    "                title=f\"Logit Difference From Top {percentile:.1%} Neurons in each Layer\")\n",
    "\n",
    "fig.update_xaxes(showticklabels = False)\n",
    "fig.update_traces(text = top_neuron_logit_diffs[:, :, 0], texttemplate = \"%{text}\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are also suggestive that the MLPs are doing most of the computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Patching\n",
    "\n",
    "In this section, we get more rigorous with our analysis by employing activation patching. \n",
    "\n",
    "**An important note here**: All functions set up in this section are predicated on returning a normalised change from patching in a corrupt activation into an otherwise clean run. \n",
    "\n",
    "This differs from some other implementations in that it is a returning normalised logit difference (like in Neel's Exploratory Analysis Demo) but patching a corrupt activation into a clean run (like in Jett's Docstring work). Personally, I feel this makes most intuitive sense, we want to understand how important a single component is toward performing a task in context of the rest of the model and logits by themselves are less intuitive than normalized effects. In this sense, a negetaive number corresponds to a loss of performance (more negative = larger loss in performance), 0 corresponds to no change and a positive number corresponds to an active improvement by removing that component.\n",
    "\n",
    "We start by running two forward passes, in this case the corrupted_tokens = (clean_token with ORIGIN and PATH_START tokens replaced with a random token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tokens are just our original padded tokens\n",
    "clean_tokens = padded_tokens\n",
    "\n",
    "# Corrupted tokens are our original padded tokens with ORIGIN and PATH_START tokens replaced with random tokens\n",
    "corrupted_tokens = padded_tokens.clone()\n",
    "\n",
    "# Replace PATH_START tokens\n",
    "corrupted_tokens[:,-1] = torch.randint(0, model.cfg.d_vocab, (1,))\n",
    "\n",
    "# Replace ORIGIN tokens\n",
    "corrupted_tokens[:, -6] = torch.randint(0, model.cfg.d_vocab, (1,))\n",
    "\n",
    "# Corrupt Forward Pass\n",
    "print(f'First Corrupt Prompt: {model.to_str_tokens(corrupted_tokens[0])}\\n')\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=True)\n",
    "\n",
    "# Clean Forward Pass\n",
    "print(f'First Clean Prompt: {model.to_str_tokens(clean_tokens[0])}\\n')\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff(logits, correct_incorrect_indices=correct_incorrect_indices, pos=-1):\n",
    "    if len(logits.shape)==3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, pos, :]\n",
    "    correct_logits = logits.gather(1, correct_incorrect_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, correct_incorrect_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logit_diff = logit_diff(clean_logits, correct_incorrect_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logit_diff(corrupted_logits, correct_incorrect_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "def counting_metric(logits, answer_token_indices=correct_incorrect_indices):\n",
    "    return (logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (CLEAN_BASELINE  - CORRUPTED_BASELINE) - 1\n",
    "\n",
    "print(f\"Clean Baseline is 0: {counting_metric(clean_logits).item():.4f} - This corresponds to no loss in performance\\n\")\n",
    "print(f\"Corrupted Baseline is -1: {counting_metric(corrupted_logits).item():.4f} - This corresponds to a complete loss in performance\\n\")\n",
    "print(f'A counting metric of > 0 corresponds to a an active improvement in performance\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a function to do resdiual stream pre patching, we call this the `resid_patcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_patcher(layer: int,\n",
    "                pos: int,\n",
    "                clean_tokens: TT[\"batch\", \"pos\"],\n",
    "                corrupted_tokens: TT[\"batch\", \"pos\"] = None,\n",
    "                corrupted_cache: ActivationCache = None,\n",
    "                prepend_bos: bool = True):\n",
    "\n",
    "    if corrupted_cache is None:\n",
    "        _, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=prepend_bos)\n",
    "    \n",
    "    def patch_clean_head_vector(\n",
    "        clean_head_vector: TT[\"batch pos head_index d_head\"],\n",
    "        hook, \n",
    "        corrupted_cache):\n",
    "        clean_head_vector[:, pos, :] = corrupted_cache[hook.name][:, pos, :]\n",
    "        return clean_head_vector\n",
    "    \n",
    "    hook_fn = partial(patch_clean_head_vector, corrupted_cache=corrupted_cache)\n",
    "\n",
    "    activation_name = utils.get_act_name(\"resid_pre\", layer)\n",
    "    patched_logits = model.run_with_hooks(\n",
    "            clean_tokens,\n",
    "            fwd_hooks = [(activation_name, hook_fn)],\n",
    "            return_type = \"logits\",\n",
    "            )\n",
    "    \n",
    "    model.reset_hooks()\n",
    "\n",
    "    return patched_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply this `resid_patcher` recursively over each layer at each postion in the model to see what part of the resid stream is playing an integral role in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_resid_patching(clean_tokens: TT[\"batch\", \"pos\"],\n",
    "                            corrupted_cache: ActivationCache | None = None,\n",
    "                            pos: int = -1):\n",
    "\n",
    "    if pos:\n",
    "        resid_patches = torch.zeros(model.cfg.n_layers, 1, device = device, dtype = torch.float32)\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "                patched_logit_diff = counting_metric(resid_patcher(\n",
    "                                                            layer = layer, \n",
    "                                                            pos = pos, \n",
    "                                                            clean_tokens = clean_tokens, \n",
    "                                                            corrupted_cache = corrupted_cache\n",
    "                                                            ), correct_incorrect_indices).item()\n",
    "                resid_patches[layer] = patched_logit_diff\n",
    "\n",
    "    else:\n",
    "        all_pos = [x for x in range(len(clean_tokens[0]))]\n",
    "        resid_patches = torch.zeros(model.cfg.n_layers, len(all_pos), device = device, dtype = torch.float32)\n",
    "        for pos in all_pos:\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                patched_logit_diff = counting_metric(resid_patcher(\n",
    "                                                            layer = layer, \n",
    "                                                            pos = pos, \n",
    "                                                            clean_tokens = clean_tokens, \n",
    "                                                            corrupted_cache = corrupted_cache\n",
    "                                                            ), correct_incorrect_indices).item()\n",
    "                resid_patches[layer, pos] = patched_logit_diff\n",
    "                    \n",
    "    return resid_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_patches = recursive_resid_patching(clean_tokens, corrupted_cache, pos = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a quick function to visualize these results, `resid_patching_imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_patching_imshow(resid_patches, layer_labels, pos_labels):\n",
    "    fig = imshow(resid_patches, \n",
    "            title = 'Normalised Change in Logit Diff Performance From Patched Resid Pre at Given position', \n",
    "            aspect = \"auto\", \n",
    "            xlabel= \"Pos\", \n",
    "            ylabel= \"Layer\",\n",
    "            xticks = pos_labels,\n",
    "            yticks = layer_labels,\n",
    "            height = 720,\n",
    "            width = 720)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the visualisation easier to read by labelling the positions along the x-axis and layers along the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = model.to_str_tokens(clean_tokens[0])\n",
    "layer_labels = [f\"Layer {layer}\" for layer in range(model.cfg.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_patching_imshow(resid_patches=resid_patches,\n",
    "                      layer_labels=layer_labels,\n",
    "                      pos_labels=pos_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are suggestive of signficant computation occuring at the `ORIGIN_START` token within the model's early layers and across many layers for the `PATH_START` token.\n",
    "\n",
    "We can get much more granular than this by performing activation patching on individual attn heads.\n",
    "\n",
    "We start by restating our clean and corrupt forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrupt Forward Pass\n",
    "print(f'First Corrupt Prompt: {model.to_str_tokens(corrupted_tokens[0])}\\n')\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=True)\n",
    "\n",
    "# Clean Forward Pass\n",
    "print(f'First Clean Prompt: {model.to_str_tokens(clean_tokens[0])}\\n')\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens, prepend_bos=True)\n",
    "\n",
    "resid_patches = recursive_resid_patching(clean_tokens, corrupted_cache, pos = None)\n",
    "\n",
    "clean_logit_diff = logit_diff(clean_logits, correct_incorrect_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logit_diff(corrupted_logits, correct_incorrect_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "\n",
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "def counting_metric(logits, answer_token_indices=correct_incorrect_indices):\n",
    "    return (logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (CLEAN_BASELINE  - CORRUPTED_BASELINE) - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we define a generic function to patch over any head in the model at any specified position, we call this the `head_patcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_patcher(layer: int,\n",
    "                head_index: int,\n",
    "                pos: int,\n",
    "                clean_tokens: TT[\"batch\", \"pos\"],\n",
    "                corrupted_tokens: TT[\"batch\", \"pos\"] = None,\n",
    "                corrupted_cache: ActivationCache = None,\n",
    "                prepend_bos: bool = True,\n",
    "                comp = None):\n",
    "\n",
    "    if corrupted_cache is None:\n",
    "        _, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=prepend_bos)\n",
    "    \n",
    "    def patch_clean_head_vector(\n",
    "        clean_head_vector: TT[\"batch pos head_index d_head\"],\n",
    "        hook, \n",
    "        head_index, \n",
    "        corrupted_cache):\n",
    "        clean_head_vector[:, pos, head_index, :] = corrupted_cache[hook.name][:, pos, head_index, :]\n",
    "        return clean_head_vector\n",
    "    \n",
    "    hook_fn = partial(patch_clean_head_vector, head_index=head_index, corrupted_cache=corrupted_cache)\n",
    "\n",
    "    if comp is None:\n",
    "        activation_name = utils.get_act_name(\"z\", layer, \"attn\")\n",
    "        patched_logits = model.run_with_hooks(\n",
    "                clean_tokens,\n",
    "                fwd_hooks = [(activation_name, hook_fn)],\n",
    "                return_type = \"logits\",\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        assert comp in [\"q\", \"k\", \"v\"]\n",
    "        activation_name = f\"blocks.{layer}.attn.hook_{comp}\"\n",
    "\n",
    "    patched_logits = model.run_with_hooks(\n",
    "                    clean_tokens,\n",
    "                    fwd_hooks = [(activation_name, hook_fn)],\n",
    "                    return_type = \"logits\",\n",
    "                    )\n",
    "    \n",
    "    model.reset_hooks()\n",
    "\n",
    "    return patched_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to patch over (Layer 10, Head 0) at the `PATH_START` position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_patched_logits = head_patcher(10, 0, -1, clean_tokens, corrupted_tokens, comp = None)\n",
    "original_logit_diff = counting_metric(clean_logits, correct_incorrect_indices).item()\n",
    "patched_head_logit_diff = counting_metric(head_patched_logits, correct_incorrect_indices).item()\n",
    "print(f\"Original logit diff: {original_logit_diff:.4f}\\n\")\n",
    "print(f\"Patched logit diff: {patched_head_logit_diff:.4f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that patching over this head as a *very* small negative effect on the models ability to do the PATH_START task. Remember for reference, a result of 0 is literally no change, -1 is a complete loss in ability to do the task and a result > 0 is an active improvement in the model's ability to do the task.\n",
    "\n",
    "As before, we set up another function that calls the `head_patcher` to recursively check every head at every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_head_patching(clean_tokens: TT[\"batch\", \"pos\"],\n",
    "                            corrupted_cache: ActivationCache = None,\n",
    "                            comp: str | None = None, \n",
    "                            pos: int | None = -1):\n",
    "\n",
    "    if pos:\n",
    "        head_patches = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device = device, dtype = torch.float32)\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                patched_logit_diff = counting_metric(head_patcher(layer = layer, \n",
    "                                            head_index = head, \n",
    "                                            pos = pos, \n",
    "                                            clean_tokens = clean_tokens, \n",
    "                                            corrupted_cache = corrupted_cache, \n",
    "                                            comp = comp), correct_incorrect_indices).item()\n",
    "                head_patches[layer, head] = patched_logit_diff\n",
    "\n",
    "    else:\n",
    "        all_pos = [x for x in range(len(clean_tokens[0]))]\n",
    "        head_patches = torch.zeros(model.cfg.n_layers*model.cfg.n_heads, len(all_pos), device = device, dtype = torch.float32)\n",
    "        for pos in all_pos:\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                for head in range(model.cfg.n_heads):\n",
    "                    head_idx = layer*model.cfg.n_heads + head\n",
    "                    patched_logit_diff = counting_metric(head_patcher(layer = layer, \n",
    "                                                                head_index = head, \n",
    "                                                                pos = pos, \n",
    "                                                                clean_tokens = clean_tokens, \n",
    "                                                                corrupted_cache = corrupted_cache, \n",
    "                                                                comp = comp), correct_incorrect_indices).item()\n",
    "                    head_patches[head_idx, pos] = patched_logit_diff\n",
    "                    \n",
    "    return head_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_patches = recursive_head_patching(clean_tokens, corrupted_cache, comp = None, pos = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another generic function to make visualising the results easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_patching_imshow(head_patches, layerhead_labels, pos_labels):\n",
    "    fig = imshow(head_patches, \n",
    "            title = 'Normalised Change in Logit Diff Performance From Patched Head at Given position', \n",
    "            aspect = \"auto\", \n",
    "            xlabel= \"Pos\", \n",
    "            ylabel= \"Layer\",\n",
    "            xticks = pos_labels,\n",
    "            yticks = layerhead_labels,\n",
    "            height = 720,\n",
    "            width = 720)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a label for every head in the model for visualization of the form: Layer.Head (ie. Layer 10 Head 5 => 10.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerhead_labels = [f\"{layer}.{head}\" for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_patching_imshow(head_patches, layerhead_labels, pos_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now crease a function to collect large effects across all of these recursive runs to make it easier to pull out the most meaningful activation patching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_large_effect(patching_effect, threshold):\n",
    "    for head_idx, head_value in enumerate(patching_effect):\n",
    "        for pos_idx, pos_value in enumerate(head_value):\n",
    "            layer = str(head_idx // 12)\n",
    "            head = str(head_idx % 12)\n",
    "            # If patching over causes greater than the threshold normalized logit diff drop decrease in performance print it out\n",
    "            if pos_value < -threshold:\n",
    "                print(f'Layer {layer} Head {head} has a large negative effect if removed at position: \"{pos_labels[pos_idx]}\" with perf. change of {pos_value:.2f}.')\n",
    "            # If patching over causes greater than the threshold normalized logit diff gain in performance print it out\n",
    "            if pos_value > threshold:\n",
    "                print(f'Layer {layer} Head {head} has a large positive effect if removed at position: \"{pos_labels[pos_idx]}\" with perf. change of {pos_value:.2f}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this function on the head patching results and find that there are a few early layer heads at the `ORIGIN` token and `PATH_START` token positions that if patched over have a considerably large negative effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_large_effect(head_patches, 0.1) # Note the choice of a 0.1 threshold is arbitrary but seems to strike a balance between finding interesting results and not printing too much"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does appear to be the case that the following heads are doing meaningful computation for this task:\n",
    "\n",
    "* Layer 0 Head 0 at the `PATH_START` token\n",
    "* Layer 0 Head 2 at the `ORIGIN` token\n",
    "* Layer 0 Head 2 at the `PATH_START` token\n",
    "* Layer 0 Head 3 at the `PATH_START` token\n",
    "* Layer 0 Head 4 at the `PATH_START` token\n",
    "* Layer 1 Head 1 at the `PATH_START` token\n",
    "* Layer 3 Head 9 at the `PATH_START` token\n",
    "\n",
    "One hypothesis here is that (1.1 and 3.9) act as ORIGIN_TOKEN movers that are active on the PATH_START token and compose with the ORIGIN token (via key/values). Additionally, it may be the case Layer 0 Head 2 is passing information to these heads about the ORIGIN token but it is unclear what this looks like currently.\n",
    "\n",
    "We will undertake a similar investigation with the MLPs, first setting up a generic `mlp_patcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_patcher(layer: int, \n",
    "\t\t\t\tpos: int, \n",
    "\t\t\t\tclean_tokens: TT[\"batch\", \"pos\"],\n",
    "\t\t\t\tcorrupted_tokens: TT[\"batch\", \"pos\"] = None,\n",
    "\t\t\t\tcorrupted_cache: ActivationCache = None, \n",
    "\t\t\t\tprepend_bos=True):\n",
    "\n",
    "\tif corrupted_cache is None:\n",
    "\t\t_, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=prepend_bos)\n",
    "\n",
    "\tdef patch_clean_head_vector(\n",
    "\t\tclean_head_vector: Float[torch.Tensor, \"batch pos d_head\"],\n",
    "\t\thook, \n",
    "\t\tcorrupted_cache):\n",
    "\t\tclean_head_vector[:, pos, :] = corrupted_cache[hook.name][:, pos, :]\n",
    "\t\treturn clean_head_vector\n",
    "\n",
    "\thook_fn = partial(patch_clean_head_vector, corrupted_cache=corrupted_cache)\n",
    "\tactivation_name = f'blocks.{layer}.hook_mlp_out'\n",
    "\n",
    "\tpatched_logits = model.run_with_hooks(\n",
    "\t\t\t\t\tclean_tokens,\n",
    "\t\t\t\t\tfwd_hooks = [(activation_name, hook_fn)],\n",
    "\t\t\t\t\treturn_type = \"logits\",\n",
    "\t\t\t\t\t)\n",
    "\t\n",
    "\tmodel.reset_hooks()\n",
    "\n",
    "\treturn patched_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use our generic `mlp_patcher` to set up a function to recursively patch over each MLP at each position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_mlp_patching(clean_tokens: TT[\"batch\", \"pos\"],\n",
    "                            corrupted_cache: ActivationCache | None = None,\n",
    "                            pos: int = -1):\n",
    "\n",
    "    if pos:\n",
    "        mlp_patches = torch.zeros(model.cfg.n_layers, 1, device = device, dtype = torch.float32)\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "                patched_logit_diff = counting_metric(mlp_patcher(\n",
    "                                                            layer = layer, \n",
    "                                                            pos = pos, \n",
    "                                                            clean_tokens = clean_tokens, \n",
    "                                                            corrupted_cache = corrupted_cache\n",
    "                                                            ), correct_incorrect_indices).item()\n",
    "                mlp_patches[layer] = patched_logit_diff\n",
    "\n",
    "    else:\n",
    "        all_pos = [x for x in range(len(clean_tokens[0]))]\n",
    "        mlp_patches = torch.zeros(model.cfg.n_layers, len(all_pos), device = device, dtype = torch.float32)\n",
    "        for pos in all_pos:\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                patched_logit_diff = counting_metric(mlp_patcher(\n",
    "                                                            layer = layer, \n",
    "                                                            pos = pos, \n",
    "                                                            clean_tokens = clean_tokens, \n",
    "                                                            corrupted_cache = corrupted_cache\n",
    "                                                            ), correct_incorrect_indices).item()\n",
    "                mlp_patches[layer, pos] = patched_logit_diff\n",
    "                    \n",
    "    return mlp_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_patches = recursive_mlp_patching(clean_tokens, corrupted_cache, pos = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally setting up a similar plotting function to easily visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_patching_imshow(mlp_patches, pos_labels):\n",
    "    fig = imshow(mlp_patches, \n",
    "            title = 'Normalised Change in Logit Diff Performance From Patched MLP Block at Given position', \n",
    "            aspect = \"auto\", \n",
    "            xlabel= \"Position\", \n",
    "            ylabel= \"Layer\",\n",
    "            xticks = pos_labels,\n",
    "            height = 720,\n",
    "            width = 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_patching_imshow(mlp_patches, pos_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest the following:\n",
    "* Patching over either MLP0 or MLP1 on the ORIGIN token has a significant effect on the model's ability to do this task.\n",
    "* Patching over MLP0, MLP1, MLP2, MLP5, MLP8, MLP9, MLP10 or MLP11 on the PATH_START token has a significant effect on the model's ability to this task.\n",
    "\n",
    "We can also get check that our original activation patching was capturing all important positions by setting all of the tokens in the corrupt dataset to random tokens, this analysis will take considerably longer to run though. After this analysis, we will move back to corrupting individual tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by patching the END token with a random value from the model's vocab\n",
    "corrupted_tokens = torch.randint_like(clean_tokens, 0, model.cfg.d_vocab) \n",
    "\n",
    "# Corrupt Forward Pass\n",
    "print(f'First Corrupt Prompt: {model.to_str_tokens(corrupted_tokens[0])}\\n')\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, prepend_bos=True)\n",
    "\n",
    "# Clean Forward Pass\n",
    "print(f'First Clean Prompt: {model.to_str_tokens(clean_tokens[0])}\\n')\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens, prepend_bos=True)\n",
    "\n",
    "clean_logit_diff = logit_diff(clean_logits, correct_incorrect_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logit_diff(corrupted_logits, correct_incorrect_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "\n",
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "def counting_metric(logits, answer_token_indices=correct_incorrect_indices):\n",
    "    return (logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (CLEAN_BASELINE  - CORRUPTED_BASELINE) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_patching = recursive_head_patching(clean_tokens = clean_tokens, corrupted_cache = corrupted_cache, comp = None, pos = None)\n",
    "head_patching_imshow(head_patching, layerhead_labels = layerhead_labels, pos_labels=pos_labels)\n",
    "collect_large_effect(head_patching, 0.1) # Again arbitrary threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we find a number of heads that are important on the PATH_START and ORIGIN tokens, here we find that there is also important computation occuring at the ADJLIST_END token.\n",
    "\n",
    "Doing the same for the MLPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_patching = recursive_mlp_patching(clean_tokens = clean_tokens, corrupted_cache = corrupted_cache, pos = None)\n",
    "mlp_patching_imshow(mlp_patching, pos_labels=pos_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see important computation occuring not only on the ORIGIN and PATH_START token but also at the ADJLIST_END token.\n",
    "\n",
    "This analysis will continue by patching over each of these tokens individually, first at PATH_START, then ORIGIN and then the ADJLIST_END token.\n",
    "\n",
    "However, thus far it does still appear to the case that a significant amount of this task is being achieved by MLPs. This is counterintuitive for a task of this nature (which just involves passing information from one token (the ORIGIN token) that is always sandwiched between ORIGIN_START and ORIGIN_END to another token (the PATH_START token)). Some further behavioural analysis conducted outside of this notebook suggests that this instance of maze-transformer is not capable of doing this task on out-of-distribution examples. For this reason, the analysis will be paused here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
